{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tibed\\anaconda3\\envs\\Research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from TTS.tts.layers.xtts.tokenizer import multilingual_cleaners\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas\n",
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "def format_audio_list(audio_files, target_language=\"en\", out_path=None, buffer=0.2, eval_percentage=0.15, speaker_name=\"coqui\", gradio_progress=None):\n",
    "    audio_total_size = 0\n",
    "    # make sure that ooutput file exists\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    # print lenght of audio files\n",
    "    print(f\"Found {len(audio_files)} audio files!\")\n",
    "\n",
    "    # Loading Whisper\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "    # device = \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    print(\"Loading Whisper Model!\")\n",
    "    asr_model = WhisperModel(\"large-v2\", device=device, compute_type=\"float32\")\n",
    "\n",
    "    metadata = {\"audio_file\": [], \"text\": [], \"speaker_name\": []}\n",
    "\n",
    "    if gradio_progress is not None:\n",
    "        tqdm_object = gradio_progress.tqdm(audio_files, desc=\"Formatting...\")\n",
    "    else:\n",
    "        tqdm_object = tqdm(audio_files)\n",
    "\n",
    "    for audio_path in tqdm_object:\n",
    "        wav, sr = torchaudio.load(audio_path)\n",
    "        # stereo to mono if needed\n",
    "        if wav.size(0) != 1:\n",
    "            wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "\n",
    "        wav = wav.squeeze()\n",
    "        audio_total_size += (wav.size(-1) / sr)\n",
    "\n",
    "        print(f\"Transcribing {audio_path}...\")\n",
    "        segments, _ = asr_model.transcribe(audio_path, word_timestamps=True, language=target_language,)\n",
    "        segments = list(segments)\n",
    "        print(f\"Found {len(segments)} segments!\")\n",
    "        i = 0\n",
    "        sentence = \"\"\n",
    "        sentence_start = None\n",
    "        first_word = True\n",
    "        # added all segments words in a unique list\n",
    "        words_list = []\n",
    "        for _, segment in enumerate(segments):\n",
    "            words = list(segment.words)\n",
    "            words_list.extend(words)\n",
    "\n",
    "        # process each word\n",
    "        for word_idx, word in enumerate(words_list):\n",
    "            if first_word:\n",
    "                sentence_start = word.start\n",
    "                # If it is the first sentence, add buffer or get the begining of the file\n",
    "                if word_idx == 0:\n",
    "                    sentence_start = max(sentence_start - buffer, 0)  # Add buffer to the sentence start\n",
    "                else:\n",
    "                    # get previous sentence end\n",
    "                    previous_word_end = words_list[word_idx - 1].end\n",
    "                    # add buffer or get the silence midle between the previous sentence and the current one\n",
    "                    sentence_start = max(sentence_start - buffer, (previous_word_end + sentence_start)/2)\n",
    "\n",
    "                sentence = word.word\n",
    "                first_word = False\n",
    "            else:\n",
    "                sentence += word.word\n",
    "\n",
    "            if word.word[-1] in [\"!\", \".\", \"?\"]:\n",
    "                sentence = sentence[1:]\n",
    "                # Expand number and abbreviations plus normalization\n",
    "                # sentence = multilingual_cleaners(sentence, target_language)\n",
    "                audio_file_name, _ = os.path.splitext(os.path.basename(audio_path))\n",
    "\n",
    "                audio_file = f\"wavs/{audio_file_name}_{str(i).zfill(8)}.wav\"\n",
    "\n",
    "                # Check for the next word's existence\n",
    "                if word_idx + 1 < len(words_list):\n",
    "                    next_word_start = words_list[word_idx + 1].start\n",
    "                else:\n",
    "                    # If don't have more words it means that it is the last sentence then use the audio len as next word start\n",
    "                    next_word_start = (wav.shape[0] - 1) / sr\n",
    "\n",
    "                # Average the current word end and next word start\n",
    "                word_end = min((word.end + next_word_start) / 2, word.end + buffer)\n",
    "                \n",
    "                absoulte_path = os.path.join(out_path, audio_file)\n",
    "                os.makedirs(os.path.dirname(absoulte_path), exist_ok=True)\n",
    "                i += 1\n",
    "                first_word = True\n",
    "\n",
    "                audio = wav[int(sr*sentence_start):int(sr*word_end)].unsqueeze(0)\n",
    "                # if the audio is too short ignore it (i.e < 0.33 seconds)\n",
    "                if audio.size(-1) >= sr/3:\n",
    "                    torchaudio.save(absoulte_path,\n",
    "                        audio,\n",
    "                        sr\n",
    "                    )\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                metadata[\"audio_file\"].append(audio_file)\n",
    "                metadata[\"text\"].append(sentence)\n",
    "                metadata[\"speaker_name\"].append(speaker_name)\n",
    "\n",
    "    df = pandas.DataFrame(metadata)\n",
    "    df = df.sample(frac=1)\n",
    "    num_val_samples = int(len(df)*eval_percentage)\n",
    "\n",
    "    df_eval = df[:num_val_samples]\n",
    "    df_train = df[num_val_samples:]\n",
    "\n",
    "    df_train = df_train.sort_values('audio_file')\n",
    "    train_metadata_path = os.path.join(out_path, \"metadata_train.csv\")\n",
    "    df_train.to_csv(train_metadata_path, sep=\"|\", index=False)\n",
    "\n",
    "    eval_metadata_path = os.path.join(out_path, \"metadata_eval.csv\")\n",
    "    df_eval = df_eval.sort_values('audio_file')\n",
    "    df_eval.to_csv(eval_metadata_path, sep=\"|\", index=False)\n",
    "\n",
    "    # deallocate VRAM and RAM\n",
    "    del asr_model, df_train, df_eval, df, metadata\n",
    "    gc.collect()\n",
    "\n",
    "    return train_metadata_path, eval_metadata_path, audio_total_size\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    # clear the GPU cache\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Clearing GPU Cache...\")\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "audio_path = \"audio\\Alan_Wake_Voice_Short.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "asr_model = WhisperModel(\"large-v3\", device=device, compute_type=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 segments!\n",
      "[Word(start=0.0, end=0.14, word=' He', probability=0.062103271484375), Word(start=0.14, end=0.3, word=' was', probability=0.87890625), Word(start=0.3, end=0.62, word=' here,', probability=0.7958984375), Word(start=1.2, end=1.34, word=' in', probability=0.8759765625), Word(start=1.34, end=1.6, word=' Bright', probability=0.85888671875), Word(start=1.6, end=2.0, word=' Falls.', probability=0.6923828125)]\n"
     ]
    }
   ],
   "source": [
    "asr_model = WhisperModel(\"large-v2\", device=device, compute_type=\"float16\")\n",
    "segments, _ = asr_model.transcribe(audio_path, word_timestamps=True, language=\"en\")\n",
    "segments = list(segments)\n",
    "print(f\"Found {len(segments)} segments!\")\n",
    "print(segments[0].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 segments!\n",
      "[Word(start=0.0, end=0.1, word=' He', probability=0.9794921875), Word(start=0.1, end=0.2, word=' was', probability=1.0), Word(start=0.2, end=0.48, word=' here.', probability=1.0), Word(start=0.7, end=1.24, word=' In', probability=0.9892578125), Word(start=1.24, end=1.46, word=' Bright', probability=0.99072265625), Word(start=1.46, end=1.76, word=' Falls.', probability=0.99560546875), Word(start=2.24, end=2.8, word=' I', probability=0.9990234375), Word(start=2.8, end=2.94, word=' could', probability=1.0), Word(start=2.94, end=3.24, word=' feel', probability=1.0), Word(start=3.24, end=3.52, word=' him', probability=1.0), Word(start=3.52, end=3.62, word=' as', probability=1.0), Word(start=3.62, end=3.74, word=' a', probability=1.0), Word(start=3.74, end=4.02, word=' growing', probability=1.0), Word(start=4.02, end=4.44, word=' pressure', probability=1.0), Word(start=4.44, end=4.8, word=' in', probability=0.99951171875), Word(start=4.8, end=4.9, word=' my', probability=1.0), Word(start=4.9, end=5.24, word=' head.', probability=1.0)]\n"
     ]
    }
   ],
   "source": [
    "segments, _ = asr_model.transcribe(audio_path, word_timestamps=True, language=\"en\")\n",
    "segments = list(segments)\n",
    "print(f\"Found {len(segments)} segments!\")\n",
    "print(segments[0].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas\n",
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "def create_dataset(output_path, audio_path, target_language=\"en\", buffer=0.2, eval_percentage=0.15, speaker_name=\"coqui\"):\n",
    "    audio_files = [audio_path]\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    whisper_model = WhisperModel(\"large-v3\", device=device, compute_type=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
